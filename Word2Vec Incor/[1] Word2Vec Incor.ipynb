{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec** é um método para gerar *Word Embeddings* a partir de um corpus de texto, utilizando redes neurais.\n",
    "\n",
    "Desenvolvido por Tomas Mikolov *et al.* (Google) em 2013, é um dos métodos de geração de *word embeddings* mais populares em tarefas de processamento de linguagem natural (PLN) como análise de sentimento, tradução de textos e reconhecimento de entidades nomeadas (NER).\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Para exercitar, vamos treinar um modelo **Word2Vec** com o corpus da Scielo \n",
    "\n",
    "https://drive.google.com/file/d/1Hh_FWgKk7TGVp0cKgqy6Hw1vxylwowEQ/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1 - importanto as bibliotecas\n",
    "Vamos primeiro instalar e importar as bibliotecas que utilizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\roger\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o pré-processamento em língua portuguesa, vamos usar o pacote ```pt_core_news_sm```. Instale antes com:\n",
    "    \n",
    "```python -m spacy download pt_core_news_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passo 2 - Pré-processamento do *corpus*\n",
    "\n",
    "\n",
    "Vamos abrir o arquivo e transformá-lo em um *dataframe*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r\"incor300.txt\", \"r\", encoding='ISO-8859-1')\n",
    "df = pd.DataFrame(file)\n",
    "df.columns = ['lines']\n",
    "df = df.sort_index()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim são os textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;IDENTIFICACAO DO PACIENTE: 10000012&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;SEQUENCIA DO EVENTO: 8&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;INTERVALO EM DIAS ENTRE OS EVENTOS: 385&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#UNHIP#\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19422</th>\n",
       "      <td>- retono em 3 meses \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19423</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19424</th>\n",
       "      <td>Prescrevi: sildenafila 20 mg comp. reves. (1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19425</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19426</th>\n",
       "      <td>Solicitei: HEMOGRAMA COMPLETO + CONTAGEM DE PL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19427 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   lines\n",
       "0                <IDENTIFICACAO DO PACIENTE: 10000012>\\n\n",
       "1                             <SEQUENCIA DO EVENTO: 8>\\n\n",
       "2            <INTERVALO EM DIAS ENTRE OS EVENTOS: 385>\\n\n",
       "3                                                     \\n\n",
       "4                                              #UNHIP#\\n\n",
       "...                                                  ...\n",
       "19422                             - retono em 3 meses \\n\n",
       "19423                                                 \\n\n",
       "19424   Prescrevi: sildenafila 20 mg comp. reves. (1 ...\n",
       "19425                                                 \\n\n",
       "19426  Solicitei: HEMOGRAMA COMPLETO + CONTAGEM DE PL...\n",
       "\n",
       "[19427 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos primeiro remover os acentos;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    '''Strip accents out.'''\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "clean2 = lambda x: cleaning1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df.lines.apply(remove_accents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;IDENTIFICACAO DO PACIENTE: 10000012&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;SEQUENCIA DO EVENTO: 8&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;INTERVALO EM DIAS ENTRE OS EVENTOS: 385&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#UNHIP#\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19422</th>\n",
       "      <td>- retono em 3 meses \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19423</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19424</th>\n",
       "      <td>Prescrevi: sildenafila 20 mg comp. reves. (1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19425</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19426</th>\n",
       "      <td>Solicitei: HEMOGRAMA COMPLETO + CONTAGEM DE PL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19427 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   lines\n",
       "0                <IDENTIFICACAO DO PACIENTE: 10000012>\\n\n",
       "1                             <SEQUENCIA DO EVENTO: 8>\\n\n",
       "2            <INTERVALO EM DIAS ENTRE OS EVENTOS: 385>\\n\n",
       "3                                                     \\n\n",
       "4                                              #UNHIP#\\n\n",
       "...                                                  ...\n",
       "19422                             - retono em 3 meses \\n\n",
       "19423                                                 \\n\n",
       "19424   Prescrevi: sildenafila 20 mg comp. reves. (1 ...\n",
       "19425                                                 \\n\n",
       "19426  Solicitei: HEMOGRAMA COMPLETO + CONTAGEM DE PL...\n",
       "\n",
       "[19427 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos limpar, usando expressões regulares, tudo que não é caracter alfanumérico e passar tudo para caixa baixa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x0000017BB8E2CCF0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief_cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos lematizar e retirar as *stopwords*, para diminuir a dimensionalidade, com a biblioteca ```spacy```. Vamos usar ```spacy.pipe()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # lematizar e retirar stopwords\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # como Word2Vec usa palavras de contexto para aprender a representação vetorial de uma palavra-alvo,\n",
    "    # se uma frase tiver apenas uma ou duas palavras,\n",
    "    # o benefício para o treinamento é muito pequeno    \n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_process=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usg doppler mmiis discreto ateromatose segmento avaliar estenose inferior segmento femuro popliteo tibio fibular',\n",
       " 'ecott feve hipo difuso ve iao imi it discreto',\n",
       " '  lab ok',\n",
       " '  rnm cardiaca fe dilatacao camaras esquerdo disfuncao sistolica moderar ventriculo esquerdar fibrose mesocardica septal basal',\n",
       " None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[681:686]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2 = [str(row.strip()) for row in txt if (row != None and row.strip() != '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negar sintoma noturnos', 'vas coriza amarelar']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt2[632:634]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos remover os valores faltantes e duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>identificacao paciente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sequencia evento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intervalar dia evento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>has dx ano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dm nid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19416</th>\n",
       "      <td>tb pulmonar tratamento iniciar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19419</th>\n",
       "      <td>aguardar terminar tratamento tb discussao ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19420</th>\n",
       "      <td>associar hctz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19424</th>\n",
       "      <td>prescrever sildenafila mg comp reves comp vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19426</th>\n",
       "      <td>solicitar hemograma completar contagem plaquet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8844 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   clean\n",
       "0                                 identificacao paciente\n",
       "1                                       sequencia evento\n",
       "2                                  intervalar dia evento\n",
       "9                                             has dx ano\n",
       "10                                                dm nid\n",
       "...                                                  ...\n",
       "19416                     tb pulmonar tratamento iniciar\n",
       "19419    aguardar terminar tratamento tb discussao ci...\n",
       "19420                                      associar hctz\n",
       "19424    prescrever sildenafila mg comp reves comp vo...\n",
       "19426  solicitar hemograma completar contagem plaquet...\n",
       "\n",
       "[8844 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11990"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigramas\n",
    "O pacote ```Gensim Phrases``` detecta automaticamente frases comuns (bigramas) de uma lista de frases.\n",
    "\n",
    "Veja mais em: https://radimrehurek.com/gensim/models/phrases.html\n",
    "\n",
    "Com o método ```Phrases()```, vamos criar frases relevantes a partir da lista de frases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "sent = [row.split() for row in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Phraser()``` reduz o consumo de memória de ```phrases``` ao descartar estado do modelo não necessário para a detecção de bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos o corpus com base nos bigramas detectados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Palavras frequentes\n",
    "Vamos calcular a frequencia das palavras, para verificar a eficácia da lematização, remoção de palavras irrelevantes e adição de bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7240"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mg',\n",
       " 'x',\n",
       " 'nao',\n",
       " 'dia',\n",
       " 'comp_comp',\n",
       " 'vos_xd',\n",
       " 'negar',\n",
       " 'usar',\n",
       " 'paciente',\n",
       " 'referir']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 3 - Treinamento do modelo\n",
    "\n",
    "Hora de treinar nosso modelo Word2Vec com nossos dados. Primeiro, vamos verificar o ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from time import time\n",
    "\n",
    "cores = multiprocessing.cpu_count() # conta o número de núcleos do computador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos parametrizar nosso modelo.\n",
    "\n",
    "- *min_count*: Ignora todas as palavras com frequência absoluta total inferior a esta\n",
    "- *window*: A distância máxima entre a palavra atual e a prevista em uma frase. Por exemplo, palavras da janela à esquerda e palavras da janela à esquerda do nosso alvo\n",
    "- *size*: Dimensionalidade dos vetores\n",
    "- *sample*: O limite para configurar quais palavras de alta frequência são reduzidas aleatoriamente\n",
    "- *alpha*: A taxa de aprendizagem inicial\n",
    "- *min_alpha*: A taxa de aprendizado cairá linearmente para *min_alpha* conforme o treinamento progride\n",
    "- *negative*: Se > 0, a amostragem negativa será usada, o int para negativo especifica quantas \"palavras de ruído\" devem ser eliminadas. Se definido como 0, nenhuma amostra negativa é usada\n",
    "- *workers*: Quantidade de *threads* de trabalho para treinar o modelo (treinamento mais rápido com máquinas multicore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=2,\n",
    "                     vector_size=32,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=10,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construindo o vocabulário e treinando o modelo\n",
    "Parâmetros do treinamento:\n",
    "\n",
    "*total_examples: Contagem de sentenças;\n",
    "*epochs*: Número de iterações (épocas) no corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(917995, 2189010)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 4 - Salvando o modelo\n",
    "Vamos salvar o modelo nos formatos *KeyedVectors* e binário, para utilizarmos posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-bc56ad85d6ac>:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)  # deixa o modelo mais eficiente, pois não vamos mais treiná-lo futuramente\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)  # deixa o modelo mais eficiente, pois não vamos mais treiná-lo futuramente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"corpus_incor.model\")\n",
    "w2v_model.wv.save_word2vec_format('corpus_incor.bin', binary=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já treinamos nosso modelo, [vamos ver aqui como utilizá-lo](https://github.com/lisaterumi/word2vec-harry-potter-portugues/blob/main/%5B2%5D%20tSNE-Harry-Potter.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
